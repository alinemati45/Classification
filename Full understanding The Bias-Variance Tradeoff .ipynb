{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-01T04:14:38.258488Z",
     "start_time": "2021-08-01T04:14:38.230538Z"
    }
   },
   "source": [
    "# Full understanding The Bias-Variance Tradeoff\n",
    "\n",
    "\n",
    "https://medium.com/swlh/the-bias-variance-tradeoff-f24253c0ab45\n",
    "    \n",
    "https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229\n",
    "    \n",
    "https://elitedatascience.com/bias-variance-tradeoff\n",
    "    \n",
    "    \n",
    "https://searchenterpriseai.techtarget.com/feature/6-ways-to-reduce-different-types-of-bias-in-machine-learning\n",
    "    \n",
    "https://towardsdatascience.com/contents-9b2e49f49fe9\n",
    "    \n",
    "https://machinelearningmastery.com/how-to-reduce-model-variance/\n",
    "    \n",
    "    \n",
    "https://www.section.io/engineering-education/ensemble-bias-var/\n",
    "    \n",
    "    \n",
    "https://www.cs.cmu.edu/~wcohen/10-601/bias-variance.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The types of errors in the prediction of a model are:\n",
    "* Bias error\n",
    "* Variance error\n",
    "* Irreducible error\n",
    "\n",
    "The predictive error of a model can be calculated as seen below:\n",
    "\n",
    "`Prediction error = (Bias error)^2 + Variance error + Irreducible error`\n",
    "\n",
    "![](./i/1_hb8F9jMk0UyYcS3jsyD8sg.png)\n",
    "If you guessed A has high Bias and B has high Variance, you’re right.\n",
    "\n",
    "\n",
    "![](./i/image43.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Error:\n",
    "When a Machine Learning model is unable to capture the true relationship between the features and target of the data, we have an error called Bias. OR Pays very little attention to the training data and oversimplifies the model. \n",
    "\n",
    "The Machine Learning model makes assumption based on the available data. If the assumption is too simple, then the model may not be able to accurately account for the relationship between the features and target of the data thereby producing inaccurate predictions.\n",
    "\n",
    "Mathematically, Bias can be defined as the difference between the Predicted values and the Expected values.\n",
    "\n",
    "![](./i/1_1R4Btn9TRhksPuIwjZTZ0g.png)\n",
    "\n",
    "Linear models such as Linear Regression and Logistic Regression which make simple assumptions have high bias. While models such as Decision Trees and Support Vector Machines have low bias.\n",
    "\n",
    "## Underfitting \n",
    "occurs when the model cannot accurately fit the training data and therefore performs poorly on training data.\n",
    "\n",
    "## Reducing Bias\n",
    "**1. Change the model(Choose the correct learning model):**\n",
    "One of the first stages to reducing Bias is to simply change the model. As stated above, some models have High bias while some do not. Do not use a Linear model if features and target of your data do not in fact have a Linear Relationship.\n",
    "\n",
    "**2. Ensure the Data is truly Representative (Use the right training dataset ):**\n",
    "\n",
    "Ensure that the training data is diverse and represents all possible groups or outcomes. In the event of an imbalanced dataset, use weighting or penalized models. There has been discussion on the poor accuracy of facial recognition models in identifying people of color. One possible source of such error is that the training dataset was not diverse and the model did not have enough training data to clearly identify persons of color.\n",
    "\n",
    "**3. Parameter tuning: **\n",
    "\n",
    "This requires an understanding of the model and model parameters. Algorithms documentations are a good place to start. Every model has a list of parameters which it takes as inputs. Tweaking these parameters may give you the desired results. You can also build your own algorithms from scratch.\n",
    "\n",
    "**4. Perform data processing mindfully** \n",
    "\n",
    "Machine intelligence involves three types of data processing: pre-processing, in-processing, and post-processing.  When you prepare datasets in pre-processing, bias can creep in during formatting before it is fed in the neural network. Any data that could introduce a bias should be excluded in this step. With in-processing, the data is manipulated as it passes through the neural network itself – so, the weighting of the neural nodes must be correct to prevent a biased output. Finally, ensure there is no bias when interpreting data for human-readable consumption in the post-processing stage. \n",
    "**5. Monitor real-world performance across the ML lifecycle **\n",
    "\n",
    "No matter how carefully you choose the learning model or vet the training data, the real-world can throw up unexpected challenges. It is important to not consider any ML model as “trained” and finalized, not requiring any further monitoring. Also, try and use real-world data for testing ML wherever possible so that bias can be detected and corrected before it creates a situation affecting human lives negatively.\n",
    "\n",
    "**6. Make sure that there are no infrastructural issues:**\n",
    "\n",
    "Apart from data and the human factor, the infrastructure itself could cause bias. For example, if you rely on data collected via electronic or mechanical sensors, then equipment problems can introduce bias. This is often the hardest type of bias to detect and needs careful consideration, with investment in the latest digital and technology infrastructure.  These five best practices should form the starting point in the discussion around bias in machine learning. \n",
    "\n",
    "## Bias is as a result of over simplified model assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance\n",
    "Pays too much attention to training data and does not generalize on the data.\n",
    "Variability of a model prediction for a given data point. We can build the model multiple\n",
    "times, so the variance is how much the predictions for a given point vary between different realizations of the model.![]\n",
    "\n",
    "## Overfitting \n",
    "occurs when a model has high variance and low bias. When a model fits too well with the training dataset such that it captures noise, it is said to have Overfit the training data. This will negatively impact the predictive power of the model. \n",
    "\n",
    "If our model returns a high accuracy on training data but performs poorly on testing data, we can denote that the model has fit too closely to the training data and can therefore not generalize on new data.\n",
    "## You can measure both types of variance in your specific model using your training data.\n",
    "\n",
    "### Measure Algorithm Variance: \n",
    "The variance introduced by the stochastic nature of the algorithm can be measured by repeating the evaluation of the algorithm on the same training dataset and calculating the variance or standard deviation of the model skill.\n",
    "### Measure Training Data Variance: \n",
    "The variance introduced by the training data can be measured by repeating the evaluation of the algorithm on different samples of training data, but keeping the seed for the pseudorandom number generator fixed then calculating the variance or standard deviation of the model skill.\n",
    "\n",
    "## Reducing Variance Error\n",
    "\n",
    "**1. Ensemble Learning:**\n",
    "\n",
    "A good way to tackle high variance is to train your data using multiple models. Ensemble learning is able to leverage on both weak and strong learners in order to improve model prediction. In fact, most winning solutions in Machine Learning \n",
    "competitions make use of Ensemble Learning.\n",
    "\n",
    "**2. Ensemble Parameters from Final Models.**\n",
    "\n",
    "**3. Increase Training Dataset Size: **\n",
    "\n",
    "This sounds tricky. Why add more data when the variance is high? More data increases the data to noise ratio which reduces the variance of the model. Also, when the model has more data, it is better able to come up with a general rule which will also apply to new data.\n",
    "\n",
    "**4. Decrease regularization :**\n",
    "\n",
    "regularization is the process of adding information (an additional penalty ) in order to solve an ill-posed problem or to prevent overfitting. Regularization makes the parameter values small and this prevents overfitting. Later in the post, we’ll see why does this work. Regularization is typically used to reduce the variance with a model by applying a penalty to the input parameters with the larger coefficients. There are a number of different methods, such as L1 regularization, Lasso regularization, dropout, etc., which help to reduce the noise and outliers within a model. However, if the data features become too uniform, the model is unable to identify the dominant trend, leading to underfitting. By decreasing the amount of regularization, more complexity and variation is introduced into the model, allowing for successful training of the model.\n",
    "\n",
    "## Variance occurs when the assumptions are too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Best: low Variance, low bias\n",
    "High bias (no attention to detail) : \n",
    "1. Underfitting \n",
    "2. Overly-simplified Model \n",
    "3. High error on both test and train data\n",
    "\n",
    "![](./i/image12.png)\n",
    "\n",
    "## high Variance (too much attention to train):\n",
    " Overfitting, Low error on train data and high on test, Starts modelling the noise in the input\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is Bias Variance Tradeoff?\n",
    "\n",
    "If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if our model has large number of parameters then it’s going to have high variance and low bias. So we need to find the right/good balance without overfitting and underfitting the data.\n",
    "\n",
    "**This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.**\n",
    "\n",
    "![](./i/image30.png)\n",
    "\n",
    "\n",
    "### Reason 1:\n",
    "Low variance (high bias) algorithms tend to be less complex, with simple or rigid underlying structure.\n",
    "\n",
    "They train models that are consistent, but inaccurate on average.\n",
    "These include linear or parametric algorithms such as regression and naive Bayes.\n",
    "\n",
    "\n",
    "\n",
    "### Reason 2:\n",
    "On the other hand, low bias (high variance) algorithms tend to be more complex, with flexible underlying structure.\n",
    "\n",
    "They train models that are accurate on average, but inconsistent.\n",
    "These include non-linear or non-parametric algorithms such as decision trees and nearest neighbors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For example: \n",
    "    \n",
    "Voting Republican - 13 Voting Democratic - 16 Non-Respondent - 21 Total - 50\n",
    "The probability of voting Republican is 13/(13+16), or 44.8%. We put out our press release that the\n",
    "Democrats are going to win by over 10 points; but, when the election comes around, it turns out they\n",
    "lose by 10 points. That certainly reflects poorly on us. Where did we go wrong in our model?\n",
    "## Bias scenario's: \n",
    "using a phonebook to select participants in our survey is one of our sources of bias.\n",
    "By only surveying certain classes of people, it skews the results in a way that will be consistent if we repeated the entire model building exercise. Similarly, not following up with respondents is another source of bias, as it consistently changes the mixture of responses we get. On our bulls-eye diagram, these move us away from the center of the target, but they would not result in an increased scatter of estimates.\n",
    "## Variance scenarios: \n",
    "the small sample size is a source of variance. If we increased our sample size, the results would be more consistent each time we repeated the survey and prediction. The results still might be highly inaccurate due to our large sources of bias, but the variance of predictions will be reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A proper machine learning workflow includes:\n",
    "\n",
    "1. Separate training and test sets\n",
    "2. Trying appropriate algorithms (No Free Lunch)\n",
    "3. Fitting model parameters\n",
    "4. Tuning impactful hyperparameters\n",
    "5. Proper performance metrics\n",
    "6. Systematic cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
